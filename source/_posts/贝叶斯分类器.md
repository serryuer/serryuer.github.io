---
title: 贝叶斯分类器
date: 2019-04-04 02:56:48
tags: 
- Machine Learning
- Notes
category:
- Mahcine Learning
---

<!-- TOC -->

- [1 贝叶斯决策论](#1-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E8%AE%BA)

<!-- /TOC -->

## 1 贝叶斯决策论

贝叶斯决策论是概率框架下实施决策的基本方法，对于分类来说，就是在所有相关概率已知的情况下，基于这些**概率**和**误判的损失**来选择最优的标记。

假设有N种可能的类别标记，即$Y=\{c_1, c_2, ..., c_N\}$，$\lambda_{ij}$是将一个真实标记为$c_j$的样本标记为$c_i$所产生的损失，那么由于我们现在只是知道所有相关的概率，比如某个类别下某个属性值的比例，我们可以得到样本$x$属于各个类别的概率，结合误判损失，我们可以得到样本$x$的期望损失：
$$R(c_i|x)=\sum_{j=1}^N\lambda_{ij}P(c_j|x)$$
$\sum$符号里面的每一项表示当样本$x$的实际标签是$c_j$的时候，样本被标记为$c_i$的损失，因为我们在这里并不知道样本的真实标签是什么，所以是将所有的损失加起来作为样本的期望损失。

我们的任务是寻找一个判定准则$h:X\rightarrow Y$，以最小化总体损失：
$$R(h)=E_x[R(h(x)|x)]$$

即：
$$h^*=\arg min_{c\in Y}R(c|x)$$

具体来说，如果分类的目标是最小化错误率，并且每一个错误的权重都是一样的（实际上不同的错误应该给予不一样的权重，比如医生的误诊和漏诊的后果严重性就不一样，应该给予不同的惩罚系数），那么误判损失$\lambda_{ij}$可以写成：
$$
\lambda_{ij}=
\begin{cases}
0,\ if\ i\ =\ j\ (表示我们的分类标记和真实的分类标记是一致的);\\
1,\ otherwise,\ (当我们的分类标签和真实标记不一致的时候的惩罚系数)
\end{cases}
$$

此时条件风险可以写成：
$$R(c|x) = 1 - P(c|x)$$

所以我们的目标可以转化成求使得后验概率$P(c|x)$最大的类别标记。

这里就引出了一个新的问题，在现实任务重我们很难直接获得后验概率$P(c|x)$，所以机器学习的目的就是根据有限的样本估计出尽量准确的$P(c|x)$，主要有两种策略：
1. 给定x，可以通过直接建模$p(c|x)$来预测c，这样得到的模型是判别模型；

2. 先对联合概率$p(x, c)$建模，然后由此获得$p(c|x)$，得到的是生成模型。
  对于生成模型来说，必然考虑：
  $$p(c|x)=\frac{p(x|c)p(c)}{p(x)}$$

  

其中，$p(x)$

